# -*- coding: utf-8 -*-
"""SpeechRecognization using rnn and lstm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sQ491hzaQ-HKsoISA7a10A54GB3ktGuT
"""

import numpy as np
import pandas as pd
import os
import librosa
import wave
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score

!pip install keras

!pip install tensorflow

!pip install tf.keras

!pip install RMSprop

import keras
import tensorflow as tf

import tensorflow as tf
from tensorflow.keras.optimizers import RMSprop

from tensorflow.keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import *
from keras.optimizers import RMSprop

from google.colab import drive
drive.mount('/content/drive')

def extract_mfcc(wav_file_name):
 y, sr =librosa.load(wav_file_name)
 mfccs = np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0)
 return mfccs

## lload radvess speeech data
ravdess_speech_labels = []
ravdess_speech_data = []
for dirname, _,filenames in os.walk('/content/drive/MyDrive/ravdess-emotional-speech-audio'):
  for filename in filenames:
   ravdess_speech_labels.append(int(filename[7:8])-1)  #index 7 and 8 rep emotion label
   wav_file_name = os.path.join(dirname,filename)
   ravdess_speech_data.append(extract_mfcc(wav_file_name))

ravdess_speech_data

import numpy as np
from tensorflow.keras.utils import to_categorical

ravdess_speech_data_array=np.asarray(ravdess_speech_data) #convert input to arrayy
ravdess_speech_label_array=np.array(ravdess_speech_labels)
ravdess_speech_label_array.shape

ravdess_speech_data_array = np.array(ravdess_speech_data) #convert input to array
ravdess_speech_label_array = np.array(ravdess_speech_labels)


labels_categorical = to_categorical(ravdess_speech_label_array)
labels_categorical.shape
ravdess_speech_data_array

ravdess_speech_data_array.shape

x_train,x_test,y_train,y_test=train_test_split(np.array(ravdess_speech_data_array),labels_categorical,test_size=0.20,random_state=9)

number_of_samples=ravdess_speech_data_array.shape[0]
training_samples = int(number_of_samples * 0.8)
validation_samples = int(number_of_samples * 0.1)
test_samples = int(number_of_samples * 0.1)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Activation

#LSTM MODEL
def create_model_LSTM():
    model = Sequential()
    model.add(LSTM(128, return_sequences=False, input_shape=(40,1)))
    model.add(Dropout(0.4))
    model.add(Activation('relu'))
    model.add(Dense(32))
    model.add(Dropout(0.4))
    model.add(Activation('relu'))
    model.add(Dense(8))
    model.add(Activation('softmax'))

    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])
    return model

w = np.expand_dims(ravdess_speech_data_array[:training_samples],-1)

w.shape

from tensorflow.keras.losses import CategoricalCrossentropy

model_A = create_model_LSTM()
history = model_A.fit(np.expand_dims(ravdess_speech_data_array[:training_samples],-1), labels_categorical[:training_samples], validation_data=(np.expand_dims(ravdess_speech_data_array[training_samples:training_samples+validation_samples], -1), labels_categorical[training_samples:training_samples+validation_samples]), epochs=130, shuffle=True)

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(loss) + 1)

plt.plot(epochs, loss, 'ro', label='Training loss')
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

#acccuracy plot using lstm

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

plt.plot(epochs, loss, 'ro', label='Training acc')
plt.plot(epochs, val_loss, 'b', label='Validation acc')
plt.title('Training and Validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.show()

#evaluate
model_A.evaluate(np.expand_dims(ravdess_speech_data_array[training_samples + validation_samples:] , -1), labels_categorical[training_samples + validation_samples:])

emotions={1 : 'neutral' , 2 : 'calm' , 3 : 'happy' , 4 :'sad', 5 : 'angry', 6 : 'fearful' , 7 :'disgust' , 8 : 'surprised'}
def predict(wav_filepath):
  test_point=extract_mfcc(wav_filepath)
  test_point=np.reshape(test_point, newshape=(1,40,1))
  predictions=model_A.predict(test_point)
  print(emotions[np.argmax(predictions[0])+1])

predict('/content/drive/MyDrive/ravdess-emotional-speech-audio/Actor_02/03-01-01-01-01-02-02.wav')

predict('/content/drive/MyDrive/ravdess-emotional-speech-audio/Actor_05/03-01-02-01-01-01-05.wav')

model_A.save('myprojectmodel.h5')

modelc=tf.keras.models.load_model('myprojectmodel.h5')